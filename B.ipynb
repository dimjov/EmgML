{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01cf7026-1396-421a-b69c-699f963e5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "!pip install -q tensorflow\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af510695-5597-4c99-85f6-8f3cec8337b0",
   "metadata": {},
   "source": [
    "UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7618f3d1-2051-4b68-8102-6d31ab62b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Method for adding +4 dead channels to the wrist channels, \n",
    "   so they match the forearm shape.  (1229,12) → (1229,16)\n",
    "\"\"\"\n",
    "def pad_to_16ch(windows):\n",
    "    if windows.shape[-1] == 16:\n",
    "        return windows\n",
    "    padded = np.zeros((windows.shape[0], windows.shape[1], 16), dtype=np.float32)\n",
    "    padded[:, :, :windows.shape[-1]] = windows\n",
    "    return padded\n",
    "\n",
    "def normalize_batch(batch):\n",
    "    # Z-score normalize per batch to avoid global mean memory issues\n",
    "    mean = np.mean(batch, axis=(0, 1), keepdims=True)\n",
    "    std = np.std(batch, axis=(0, 1), keepdims=True) + 1e-8\n",
    "    return (batch - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79f4bedc-837b-437e-aa2e-17a52c727078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\"\"\" Data Generator (loads npz files in small chunks) \"\"\"\n",
    "class EMGDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, files, batch_size=64, steps_per_epoch=1000):\n",
    "        self.files = files\n",
    "        self.batch_size = batch_size\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_batch, y_batch = [], []\n",
    "        while len(X_batch) < self.batch_size:\n",
    "            f = random.choice(self.files)\n",
    "            d = np.load(f)\n",
    "            # Randomly pick forearm or wrist from this file\n",
    "            if random.random() < 0.5 and len(d['forearm_windows']) > 0:\n",
    "                x = d['forearm_windows']\n",
    "                y = d['forearm_labels'] - 1\n",
    "            else:\n",
    "                x = pad_to_16ch(d['wrist_windows'])\n",
    "                y = d['wrist_labels'] - 1\n",
    "            if len(x) == 0: \n",
    "                continue\n",
    "            i = np.random.randint(0, len(x))  # random window from that file\n",
    "            X_batch.append(x[i])\n",
    "            y_batch.append(y[i])\n",
    "        X_batch = np.array(X_batch, dtype=np.float32)\n",
    "        y_batch = np.array(y_batch, dtype=np.int32)\n",
    "        # normalize batch\n",
    "        X_batch = normalize_batch(X_batch)\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfb4bfd2-0e55-466e-97b1-a14ee422abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CNN feature extractor ----------\n",
    "\n",
    "cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, 5, dilation_rate=1, activation='relu', input_shape=(1229, 16)),\n",
    "    tf.keras.layers.Conv1D(128, 5, dilation_rate=2, activation='relu'),\n",
    "    tf.keras.layers.Conv1D(256, 3, dilation_rate=4, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D()  # -> (batch, 256)\n",
    "])\n",
    "\n",
    "# Add classification head temporarily for training\n",
    "model = tf.keras.Sequential([\n",
    "    cnn,\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(17, activation='softmax')  # 16 gestures + 1 rest\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a4f93-2e5b-417d-aca5-3d84b1b64bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m   1/1000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14:20:06\u001b[0m 52s/step - accuracy: 0.0156 - loss: 2.9465"
     ]
    }
   ],
   "source": [
    "# ---------- Collect file paths & train CNN ----------\n",
    "\n",
    "data_dir = r\".\\Processed_data\"\n",
    "all_files = [os.path.join(dp, f)\n",
    "             for dp, dn, fn in os.walk(data_dir)\n",
    "             for f in fn if f.endswith(\".npz\")]\n",
    "\n",
    "# Split file list into train/test (80/20)\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "train_gen = EMGDataGenerator(train_files, batch_size=64, steps_per_epoch=1000)\n",
    "test_gen = EMGDataGenerator(test_files, batch_size=64, steps_per_epoch=200)\n",
    "\n",
    "# Train the CNN\n",
    "model.fit(train_gen, validation_data=test_gen, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a939b71-eb4a-43c6-b02c-a482c00a25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Freeze CNN and extract features ----------\n",
    "\n",
    "for layer in cnn.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "def extract_features(files):\n",
    "    X_feats, y_all = [], []\n",
    "    for f in files:\n",
    "        d = np.load(f)\n",
    "        fw, fl = d['forearm_windows'], d['forearm_labels']\n",
    "        ww, wl = d['wrist_windows'], d['wrist_labels']\n",
    "        if len(fw) > 0:\n",
    "            fw = normalize_batch(fw)\n",
    "            feats = cnn.predict(fw, batch_size=64, verbose=0)\n",
    "            X_feats.append(feats)\n",
    "            y_all.append(fl)\n",
    "        if len(ww) > 0:\n",
    "            ww = normalize_batch(pad_to_16ch(ww))\n",
    "            feats = cnn.predict(ww, batch_size=64, verbose=0)\n",
    "            X_feats.append(feats)\n",
    "            y_all.append(wl)\n",
    "    return np.concatenate(X_feats, axis=0), np.concatenate(y_all, axis=0)\n",
    "\n",
    "train_feats, y_train = extract_features(train_files)\n",
    "test_feats, y_test = extract_features(test_files)\n",
    "\n",
    "print(\"Feature shapes:\", train_feats.shape, test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfa350-1e97-4cc5-a83d-d343abfb82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Train and evaluate SVM ----------\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_feats)\n",
    "test_scaled = scaler.transform(test_feats)\n",
    "\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "svm.fit(train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe740803-7196-48a2-a29a-05c773241651",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(test_scaled)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1-score :\", f1)\n",
    "print(\"\\nFull classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa512f45-6411-4440-a7b8-3d7dc29a24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b43bb-3da9-4aac-b648-40a430b54225",
   "metadata": {},
   "source": [
    "LOAD ALL FOREARM + WRIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adc4c76f-0647-4846-99c0-35d1ed0146b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Method for adding 4 aditional dead channels to the wrist channels, \n",
    "   so they match the forearm shape.  (1229,12) → (1229,16)\n",
    "\"\"\"\n",
    "def pad_to_16ch(windows):\n",
    "    if windows.shape[-1] == 16:\n",
    "        return windows\n",
    "    padded = np.zeros((windows.shape[0], windows.shape[1], 16), dtype=np.float32)\n",
    "    padded[:, :, :windows.shape[-1]] = windows\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20b81c26-9b48-4cef-8801-e623ece2c8cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 33.7 GiB for an array with shape (460530, 1228, 16) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m             X_list\u001b[38;5;241m.\u001b[39mappend(ww_padded)\n\u001b[0;32m     24\u001b[0m             y_list\u001b[38;5;241m.\u001b[39mappend(wl)\n\u001b[1;32m---> 26\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(X_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)   \u001b[38;5;66;03m# (N, 1229, 16)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(y_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)   \u001b[38;5;66;03m# (N,)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 33.7 GiB for an array with shape (460530, 1228, 16) and data type float32"
     ]
    }
   ],
   "source": [
    "data_dir = r\".\\Processed_data\"\n",
    "X_list, y_list = [], []\n",
    "\n",
    "for session_folder in os.listdir(data_dir):\n",
    "    session_path = os.path.join(data_dir, session_folder)\n",
    "    for file in os.listdir(session_path):\n",
    "        if not file.endswith(\".npz\"):\n",
    "            continue\n",
    "        d = np.load(os.path.join(session_path, file))\n",
    "\n",
    "        # Forearm\n",
    "        fw = d['forearm_windows']\n",
    "        fl = d['forearm_labels'] - 1\n",
    "        if len(fw) > 0:\n",
    "            X_list.append(fw)\n",
    "            y_list.append(fl)\n",
    "\n",
    "        # Wrist\n",
    "        ww = d['wrist_windows']\n",
    "        wl = d['wrist_labels'] - 1\n",
    "        if len(ww) > 0:\n",
    "            ww_padded = pad_to_16ch(ww)\n",
    "            X_list.append(ww_padded)\n",
    "            y_list.append(wl)\n",
    "\n",
    "X = np.concatenate(X_list, axis=0)   # (N, 1229, 16)\n",
    "y = np.concatenate(y_list, axis=0)   # (N,)\n",
    "print(\"Loaded data:\", X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247a646-bfc4-43d9-af62-2c1fc7abaaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional normalization (z-score per channel)\n",
    "X = (X - np.mean(X, axis=(0,1))) / (np.std(X, axis=(0,1)) + 1e-8)\n",
    "\n",
    "# Do a train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5f55a-bd06-4769-8762-a99b939cbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 1D CNN feature extractor\n",
    "cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, dilation_rate=1, activation='relu', input_shape=(1229,16)),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, dilation_rate=2, activation='relu'),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=3, dilation_rate=4, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D()  # -> (batch, 256)\n",
    "])\n",
    "\n",
    "# Build a temp classification head for CNN training only\n",
    "model = tf.keras.Sequential([\n",
    "    cnn,\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b066a41-cc5e-41fd-9dd1-81e7db20ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN feature extractor\n",
    "model.fit(X_train, y_train, validation_split=0.1, epochs=10, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa000a6b-3a6c-4c42-b98c-e4f8cf97c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze CNN and extract features\n",
    "for layer in cnn.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ff5e1-36da-489b-9170-5e27b4677b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = cnn.predict(X_train, batch_size=64)\n",
    "test_features = cnn.predict(X_test, batch_size=64)\n",
    "print(\"Feature shape:\", train_features.shape)  # (N_train, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf108ec0-7c2f-4cab-9e48-028997ad9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on extracted features\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_features)\n",
    "test_scaled = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd0d7f-1ba6-4d52-b147-344ee9b369fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale')\n",
    "svm.fit(train_scaled, y_train)\n",
    "\n",
    "# Evaluate the SVM model\n",
    "y_pred = svm.predict(test_scaled)\n",
    "\n",
    "# metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Accuracy :\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall   :\", rec)\n",
    "print(\"F1-score :\", f1)\n",
    "print(\"\\nFull classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmgEnv",
   "language": "python",
   "name": "emgenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
