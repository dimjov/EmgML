{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc542a42-4a5c-4cd1-9682-7f171056b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "!pip install -q wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb  # MIT-BIH compatible reader\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37d4e6a-c2a9-4d2a-8404-11f7a0d0b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset constants\n",
    "NUM_OF_PARTICIPANTS = 43\n",
    "NUM_OF_SESSIONS = 3\n",
    "NUM_OF_GESTURES = 16\n",
    "NUM_OF_TRIALS = 7\n",
    "FS = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6db8df4b-9a9f-4b38-ab84-58941aa6e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "\n",
    "\"\"\"Bandpass filter to keep only EMG band.\"\"\"\n",
    "def _bandpass_filter(data, low=20, high=450, fs=2048, order=4):\n",
    "    nyq = fs / 2\n",
    "    b, a = butter(order, [low / nyq, high / nyq], btype='band')\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "\"\"\"Notch filter to remove powerline noise (60Hz).\"\"\"\n",
    "def _notch_filter(data, freq=60.0, fs=2048, quality=30):\n",
    "    \"\"\"Notch filter to remove powerline noise (60 Hz).\"\"\"\n",
    "    nyq = fs / 2\n",
    "    w0 = freq / nyq\n",
    "    b, a = iirnotch(w0, quality)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "\"\"\"High-pass filter to remove DC offset / drift.\"\"\"\n",
    "def _dc_removal(data, cutoff=0.1, fs=2048, order=2):\n",
    "    nyq = fs / 2\n",
    "    b, a = butter(order, cutoff / nyq, btype='high')\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "\"\"\"Apply DC removal + 60Hz notch + bandpass in order.\"\"\"\n",
    "def full_preprocess(data, fs=2048):\n",
    "    x = _dc_removal(data, fs=fs)\n",
    "    x = _notch_filter(x, fs=fs)\n",
    "    x = _bandpass_filter(x, fs=fs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1826a3-431e-4f03-a656-449e169b179f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: 1 of 129 participants\n",
      "Converted: 2 of 129 participants\n",
      "Converted: 3 of 129 participants\n",
      "Converted: 4 of 129 participants\n",
      "Converted: 5 of 129 participants\n",
      "Converted: 6 of 129 participants\n",
      "Converted: 7 of 129 participants\n",
      "Converted: 8 of 129 participants\n",
      "Converted: 9 of 129 participants\n",
      "Converted: 10 of 129 participants\n",
      "Converted: 11 of 129 participants\n",
      "Converted: 12 of 129 participants\n",
      "Converted: 13 of 129 participants\n",
      "Converted: 14 of 129 participants\n",
      "Converted: 15 of 129 participants\n",
      "Converted: 16 of 129 participants\n",
      "Converted: 17 of 129 participants\n",
      "Converted: 18 of 129 participants\n",
      "Converted: 19 of 129 participants\n",
      "Converted: 20 of 129 participants\n",
      "Converted: 21 of 129 participants\n",
      "Converted: 22 of 129 participants\n",
      "Converted: 23 of 129 participants\n",
      "Converted: 24 of 129 participants\n",
      "Converted: 25 of 129 participants\n",
      "Converted: 26 of 129 participants\n",
      "Converted: 27 of 129 participants\n",
      "Converted: 28 of 129 participants\n",
      "Converted: 29 of 129 participants\n",
      "Converted: 30 of 129 participants\n",
      "Converted: 31 of 129 participants\n",
      "Converted: 32 of 129 participants\n",
      "Converted: 33 of 129 participants\n",
      "Converted: 34 of 129 participants\n",
      "Converted: 35 of 129 participants\n",
      "Converted: 36 of 129 participants\n",
      "Converted: 37 of 129 participants\n",
      "Converted: 38 of 129 participants\n",
      "Converted: 39 of 129 participants\n",
      "Converted: 40 of 129 participants\n",
      "Converted: 41 of 129 participants\n",
      "Converted: 42 of 129 participants\n",
      "Converted: 43 of 129 participants\n",
      "Converted: 44 of 129 participants\n",
      "Converted: 45 of 129 participants\n",
      "Converted: 46 of 129 participants\n",
      "Converted: 47 of 129 participants\n",
      "Converted: 48 of 129 participants\n",
      "Converted: 49 of 129 participants\n",
      "Converted: 50 of 129 participants\n",
      "Converted: 51 of 129 participants\n",
      "Converted: 52 of 129 participants\n",
      "Converted: 53 of 129 participants\n",
      "Converted: 54 of 129 participants\n",
      "Converted: 55 of 129 participants\n",
      "Converted: 56 of 129 participants\n",
      "Converted: 57 of 129 participants\n",
      "Converted: 58 of 129 participants\n",
      "Converted: 59 of 129 participants\n",
      "Converted: 60 of 129 participants\n",
      "Converted: 61 of 129 participants\n",
      "Converted: 62 of 129 participants\n",
      "Converted: 63 of 129 participants\n",
      "Converted: 64 of 129 participants\n",
      "Converted: 65 of 129 participants\n",
      "Converted: 66 of 129 participants\n",
      "Converted: 67 of 129 participants\n",
      "Converted: 68 of 129 participants\n",
      "Converted: 69 of 129 participants\n",
      "Converted: 70 of 129 participants\n",
      "Converted: 71 of 129 participants\n",
      "Converted: 72 of 129 participants\n",
      "Converted: 73 of 129 participants\n",
      "Converted: 74 of 129 participants\n",
      "Converted: 75 of 129 participants\n",
      "Converted: 76 of 129 participants\n",
      "Converted: 77 of 129 participants\n",
      "Converted: 78 of 129 participants\n",
      "Converted: 79 of 129 participants\n",
      "Converted: 80 of 129 participants\n",
      "Converted: 81 of 129 participants\n",
      "Converted: 82 of 129 participants\n",
      "Converted: 83 of 129 participants\n",
      "Converted: 84 of 129 participants\n",
      "Converted: 85 of 129 participants\n",
      "Converted: 86 of 129 participants\n",
      "Converted: 87 of 129 participants\n",
      "Converted: 88 of 129 participants\n",
      "Converted: 89 of 129 participants\n",
      "Converted: 90 of 129 participants\n",
      "Converted: 91 of 129 participants\n",
      "Converted: 92 of 129 participants\n",
      "Converted: 93 of 129 participants\n",
      "Converted: 94 of 129 participants\n",
      "Converted: 95 of 129 participants\n",
      "Converted: 96 of 129 participants\n",
      "Converted: 97 of 129 participants\n",
      "Converted: 98 of 129 participants\n",
      "Converted: 99 of 129 participants\n",
      "Converted: 100 of 129 participants\n",
      "Converted: 101 of 129 participants\n",
      "Converted: 102 of 129 participants\n",
      "Converted: 103 of 129 participants\n",
      "Converted: 104 of 129 participants\n",
      "Converted: 105 of 129 participants\n",
      "Converted: 106 of 129 participants\n",
      "Converted: 107 of 129 participants\n",
      "Converted: 108 of 129 participants\n",
      "Converted: 109 of 129 participants\n",
      "Converted: 110 of 129 participants\n",
      "Converted: 111 of 129 participants\n",
      "Converted: 112 of 129 participants\n",
      "Converted: 113 of 129 participants\n",
      "Converted: 114 of 129 participants\n",
      "Converted: 115 of 129 participants\n",
      "Converted: 116 of 129 participants\n",
      "Converted: 117 of 129 participants\n",
      "Converted: 118 of 129 participants\n",
      "Converted: 119 of 129 participants\n",
      "Converted: 120 of 129 participants\n",
      "Converted: 121 of 129 participants\n",
      "Converted: 122 of 129 participants\n",
      "Converted: 123 of 129 participants\n",
      "Converted: 124 of 129 participants\n",
      "Converted: 125 of 129 participants\n",
      "Converted: 126 of 129 participants\n",
      "Converted: 127 of 129 participants\n",
      "Converted: 128 of 129 participants\n",
      "Converted: 129 of 129 participants\n",
      "Participant-level conversion + preprocessing (no windowing) completed.\n"
     ]
    }
   ],
   "source": [
    "mainFolder = r\".\\\\Grabmyo-1.0.2\"\n",
    "converted_root = \"Processed_data\"\n",
    "os.makedirs(converted_root, exist_ok=True)  # dir for the processed + windowed data\n",
    "\n",
    "FOREARM_CHANNELS = list(range(0, 16))\n",
    "WRIST_CHANNELS = list(range(17, 23)) + list(range(25, 31))\n",
    "\n",
    "count = 0\n",
    "for sessionNum in range(1, NUM_OF_SESSIONS + 1):\n",
    "    sessionFolder = f\"Session{sessionNum}_Converted\"\n",
    "    sessionPath = os.path.join(converted_root, sessionFolder)\n",
    "    os.makedirs(sessionPath, exist_ok=True)\n",
    "\n",
    "    for participantNum in range(1, NUM_OF_PARTICIPANTS + 1):\n",
    "        forearmData = {}  # dict: (gesture,trial) -> ndarray (samples x 16)\n",
    "        wristData   = {}  # dict: (gesture,trial) -> ndarray (samples x 12)\n",
    "\n",
    "        participantFolder = f\"session{sessionNum}_participant{participantNum}\"\n",
    "        for gestureNum in range(1, NUM_OF_GESTURES + 2):  # +1 for resting -> 17 labels\n",
    "            for trialNum in range(1, NUM_OF_TRIALS + 1):\n",
    "                filename = f\"session{sessionNum}_participant{participantNum}_gesture{gestureNum}_trial{trialNum}\"\n",
    "                filepath = os.path.join(mainFolder, f\"Session{sessionNum}\", participantFolder, filename)\n",
    "\n",
    "                try:\n",
    "                    record = wfdb.rdrecord(filepath)\n",
    "                    emgData = record.p_signal  # shape (n_samples, n_channels)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Error reading {filepath}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Select channels and preprocess separately\n",
    "                try:\n",
    "                    forearm = emgData[:, FOREARM_CHANNELS]  # (n_samples, 16)\n",
    "                    wrist   = emgData[:, WRIST_CHANNELS]    # (n_samples, 12)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Channel indexing problem for {filepath}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                forearm_pp = full_preprocess(forearm, fs=FS)\n",
    "                wrist_pp   = full_preprocess(wrist, fs=FS)\n",
    "\n",
    "                # Save under keys (gesture, trial) - consistent ordering\n",
    "                forearmData[(gestureNum, trialNum)] = forearm_pp\n",
    "                wristData[(gestureNum, trialNum)] = wrist_pp\n",
    "\n",
    "        # Save per-participant file (preprocessed, un-windowed)\n",
    "        save_path = os.path.join(sessionPath, f\"session{sessionNum}_participant{participantNum}.npz\")\n",
    "        # storing dicts is fine (they are pickled inside npz); keeps structure for on-the-fly windowing\n",
    "        np.savez_compressed(save_path, forearmData=forearmData, wristData=wristData)\n",
    "\n",
    "        count += 1\n",
    "        print(f\"Converted: {count} of {NUM_OF_PARTICIPANTS * NUM_OF_SESSIONS} participants\")\n",
    "\n",
    "print(\"Participant-level conversion + preprocessing (no windowing) completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef8459e-c2f0-4626-9832-a6d0e6cf3735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10240, 16)\n",
      "[[ 0.00082468  0.00103293  0.00181015 ...  0.00078637  0.00120205\n",
      "   0.00115372]\n",
      " [-0.00349553 -0.00424119  0.00209671 ...  0.00094024 -0.00765628\n",
      "  -0.00499999]\n",
      " [-0.0047394  -0.00697291  0.00224367 ...  0.00445375 -0.01070339\n",
      "  -0.00771597]\n",
      " ...\n",
      " [-0.02658384 -0.02291809 -0.03653183 ... -0.01205873 -0.0282834\n",
      "  -0.01065593]\n",
      " [-0.02263637 -0.0243561  -0.04046352 ... -0.01629123 -0.0243865\n",
      "  -0.01068911]\n",
      " [-0.00376234 -0.01069089 -0.0231561  ... -0.00555721 -0.00516356\n",
      "   0.00379449]]\n"
     ]
    }
   ],
   "source": [
    "sample = np.load(r\".\\Processed_data\\Session1_Converted\\session1_participant36.npz\", allow_pickle=True)\n",
    "data = sample['forearmData'].item()[(1,1)]  # gesture 1, trial 1\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmgEnv",
   "language": "python",
   "name": "emgenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
